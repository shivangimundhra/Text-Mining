{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9939b174",
   "metadata": {},
   "source": [
    "# HW_2 MNB and SVM for Fake Review Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "228ff5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81ecce",
   "metadata": {},
   "source": [
    "# Step 1: Read in data - For both MNB and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "47fd1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = p.read_csv(\"/Users/shivangi/Downloads/deception_data_converted_final(1).tsv\", delimiter='\\t')\n",
    "y = train['sentiment'].values\n",
    "y1 = train['lie'].values\n",
    "X = train['review'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3031c0",
   "metadata": {},
   "source": [
    "# Step 2: Split train/test data for hold-out test - For both MNB and SVM\n",
    "\n",
    "we'll first work on predicting the\n",
    "1. sentiment, then\n",
    "2. authenticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "174c0c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73,) (73,) (19,) (19,)\n",
      "'I went to Applebees (regrettably) once and it was a train-wreck. The server was in a terrible mood, the beers arrived after the dinner was delivered, the appetizer was wrong, food was bad, the check was wrong, and there were barely any other people inside! I was pretty baffled at how so many things just went wrong in the whole process, the restaurant wasn\\'t even busy. '\n",
      "n\n",
      "'After I went shopping with some of my friend, we went to DODO restaurant for dinner. I found worm in one of the dishes .'\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a050894",
   "metadata": {},
   "source": [
    "# Step 2.1 Data Checking - MNB and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0719a5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['n' 'p']\n",
      " [35 38]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts = True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "409f4535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['n' 'p']\n",
      " [11 8]]\n"
     ]
    }
   ],
   "source": [
    "uniqueTest, countsTest = np.unique(y_test, return_counts=True)\n",
    "print(np.asarray((uniqueTest, countsTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456e767",
   "metadata": {},
   "source": [
    "trainRatio = 35/38 = 92%\n",
    "testRatio = 11/8 = 137.5%\n",
    "\n",
    "It's a bit lopsided in test data but that should not be a problem because training data has almost 1:1 negative and positive records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5817a",
   "metadata": {},
   "source": [
    "# Step 3: Vectorization - MNB and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a5147b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# several commonly used vectorizer setting\n",
    "\n",
    "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a7072",
   "metadata": {},
   "source": [
    "# Step 3.1: Vectorize the training data - MNB and SVM\n",
    "\n",
    "We'll use count vectorizer for sentiment because multinomialNB uses frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a6a481df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 70)\n",
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 2 0]]\n",
      "70\n",
      "[('went', 68), ('terrible', 59), ('dinner', 16), ('food', 23), ('bad', 3), ('people', 45), ('just', 31), ('restaurant', 50), ('wasn', 67), ('friends', 26)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec.shape)\n",
    "print(X_train_vec[0].toarray())\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_count_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check word index in vocabulary\n",
    "print(unigram_count_vectorizer.vocabulary_.get('imaginative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ca68b",
   "metadata": {},
   "source": [
    "# Step 3.2: Vectorize the test data - MNB and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b199765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 70)\n"
     ]
    }
   ],
   "source": [
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "# print out #examples and #features in the test set\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8912164b",
   "metadata": {},
   "source": [
    "# Step 4: Train a MNB classifier - MNB only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7525d427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the MNB module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# initialize the MNB model\n",
    "nb_clf= MultinomialNB()\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "nb_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e3849",
   "metadata": {},
   "source": [
    "# Step 4: Train a linear SVC classifier - SVM only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7da876a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the LinearSVC module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf = LinearSVC(C=1)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d4b58",
   "metadata": {},
   "source": [
    "# Step 4.1 Interpret a trained MNB model - conditional probs - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "200b1aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 70)\n",
      "3\n",
      "-4.056604234239254\n",
      "-5.424950017481403\n"
     ]
    }
   ],
   "source": [
    "print(nb_clf.feature_log_prob_.shape)\n",
    "\n",
    "print(unigram_count_vectorizer.vocabulary_.get('bad'))\n",
    "\n",
    "# for i in range(0,1):\n",
    "print(nb_clf.feature_log_prob_[0][unigram_count_vectorizer.vocabulary_.get('bad')])\n",
    "print(nb_clf.feature_log_prob_[1][unigram_count_vectorizer.vocabulary_.get('bad')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3691d0f",
   "metadata": {},
   "source": [
    "# Step 4.1 Interpret the trained linear SVC model - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "016fcd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative words\n",
      "(-0.752369028583336, 'cold')\n",
      "(-0.6548064356265509, 'dishes')\n",
      "(-0.6291139770843042, 'service')\n",
      "(-0.5566375936209397, 'said')\n",
      "(-0.5336867420691154, 'went')\n",
      "(-0.521638769856277, 'meal')\n",
      "(-0.5008944257517955, 'asked')\n",
      "(-0.48791419931992813, 'ny')\n",
      "(-0.4682526776670327, 'minutes')\n",
      "(-0.45505653803669016, 'restaurant')\n",
      "\n",
      "positive words\n",
      "(0.3538775775757943, 'ask')\n",
      "(0.3888561624326201, 'environment')\n",
      "(0.4573940627397639, 'waiters')\n",
      "(0.4662163169984037, 'great')\n",
      "(0.5612563001744503, 'fresh')\n",
      "(0.5899592689467185, 'delicious')\n",
      "(0.6048695375063617, 'nice')\n",
      "(0.6437193255535749, 'friendly')\n",
      "(0.7861998220361109, 'need')\n",
      "(0.9646194557255225, 'best')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## For category \"0\" (negative), get all features and their weights and sort them in increasing order\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], unigram_count_vectorizer.get_feature_names_out()))\n",
    "\n",
    "## get the 10 features that are best indicators of negative sentiment (they are at the bottom of the ranked list negative_feature_ranks)\n",
    "negative_10 = feature_ranks[:10]\n",
    "print(\"negative words\")\n",
    "for i in range(0, len(negative_10)):\n",
    "    print(negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are best indicators of positive sentiment (they are at the bottom of the ranked list positive_feature_ranks)\n",
    "positive_10 = feature_ranks[-10:]\n",
    "print(\"positive words\")\n",
    "for i in range(0, len(positive_10)):\n",
    "    print(positive_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed51fff",
   "metadata": {},
   "source": [
    "# Step 4.2 Log ratio of conditional probs - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fcbf6c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-2.262163659264246, 'terrible'), (-2.1668534794599212, 'asked'), (-2.0614929638020945, 'took'), (-1.9437099281457106, 'came'), (-1.9437099281457106, 'said'), (-1.4047134274130237, 'minutes'), (-1.368345783242149, 'bad'), (-1.368345783242149, 'wasn'), (-0.9628806751339853, 'cold'), (-0.9628806751339853, 'dine')]\n",
      "[(1.11656086654585, 'sauce'), (1.3884945820294927, 'delicious'), (1.3884945820294927, 'need'), (1.5220259746540146, 'nice'), (1.5220259746540146, 'prices'), (1.6398090103103984, 'great'), (1.6398090103103988, 'friendly'), (1.84047970577255, 'fresh'), (1.9683130772824344, 'best'), (2.7006809709956614, 'amazing')]\n"
     ]
    }
   ],
   "source": [
    "log_ratios = []\n",
    "features = unigram_count_vectorizer.get_feature_names_out()\n",
    "neg_cond_prob = nb_clf.feature_log_prob_[0]\n",
    "pos_cond_prob = nb_clf.feature_log_prob_[1]\n",
    "\n",
    "for i in range(0, len(features)):\n",
    "  log_ratio = pos_cond_prob[i] - neg_cond_prob[i]\n",
    "  log_ratios.append(log_ratio)\n",
    "\n",
    "exercise_C_ranks = sorted(zip(log_ratios, features))\n",
    "print(exercise_C_ranks[:10])\n",
    "print(exercise_C_ranks[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a87161",
   "metadata": {},
   "source": [
    "# Step 5: Test the MNB classifier - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bdd06cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "\n",
    "nb_clf.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f1b41622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 2]\n",
      " [0 8]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix (row: ground truth; col: prediction)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_mnb = nb_clf.fit(X_train_vec, y_train).predict(X_test_vec)\n",
    "cm_mnb = confusion_matrix(y_test, y_pred_mnb, labels = ['n', 'p'])\n",
    "print(cm_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "728c1c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  0.8]\n",
      "[0.81818182 1.        ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90        11\n",
      "           1       0.80      1.00      0.89         8\n",
      "\n",
      "    accuracy                           0.89        19\n",
      "   macro avg       0.90      0.91      0.89        19\n",
      "weighted avg       0.92      0.89      0.90        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print(precision_score(y_test, y_pred_mnb, average=None))\n",
    "print(recall_score(y_test, y_pred_mnb, average=None))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1']\n",
    "print(classification_report(y_test, y_pred_mnb, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e585043",
   "metadata": {},
   "source": [
    "# Step 5: Test the LinearSVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "30fd8c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7894736842105263"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "\n",
    "svm_clf.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9a0c4099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  1]\n",
      " [ 3  5]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83        11\n",
      "           1       0.83      0.62      0.71         8\n",
      "\n",
      "    accuracy                           0.79        19\n",
      "   macro avg       0.80      0.77      0.77        19\n",
      "weighted avg       0.80      0.79      0.78        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_svm = svm_clf.predict(X_test_vec)\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm, labels = ['n', 'p'])\n",
    "print(cm_svm)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1']\n",
    "print(classification_report(y_test, y_pred_svm, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c3eba",
   "metadata": {},
   "source": [
    "# Step 5.1 Interpret the prediction result - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ab2aa313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86168678 0.13831322]\n",
      "n\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "## find the calculated posterior probability\n",
    "posterior_probs_mnb = nb_clf.predict_proba(X_test_vec)\n",
    "\n",
    "## find the posterior probabilities for the first test example\n",
    "print(posterior_probs_mnb[0])\n",
    "\n",
    "# find the category prediction for the first test example\n",
    "y_pred_mnb = nb_clf.predict(X_test_vec)\n",
    "print(y_pred_mnb[0])\n",
    "\n",
    "# check the actual label for the first test example\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d168436",
   "metadata": {},
   "source": [
    "Because the posterior probability for category 'n' (negative) is the greatest, 0.862, the prediction should be \"n\". Because the actual label is also \"n\", this is a correct prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61e104",
   "metadata": {},
   "source": [
    "# Step 5.1 Interpret the prediction result - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "958bbdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5476709994951778\n",
      "n\n",
      "'Mike\\'s Pizza High Point, NY Service was very slow and the quality was low. You would think they would know at least how to make good pizza, not. Stick to pre-made dishes like stuffed pasta or a salad. You should consider dining else where.'\n"
     ]
    }
   ],
   "source": [
    "## get the confidence scores for all test examples from each of the five binary classifiers\n",
    "svm_confidence_scores = svm_clf.decision_function(X_test_vec)\n",
    "## get the confidence score for the first test example\n",
    "print(svm_confidence_scores[0])\n",
    "\n",
    "## Confirm by printing out the actual prediction\n",
    "print(y_test[0])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3ce7a4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7286686, 0.2713314])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "svm_calibrated = CalibratedClassifierCV(svm_clf) \n",
    "svm_calibrated.fit(X_train_vec, y_train)\n",
    "y_test_proba_svm = svm_calibrated.predict_proba(X_test_vec)\n",
    "y_test_proba_svm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb7b98",
   "metadata": {},
   "source": [
    "# Step 5.2 Error Analysis - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f054cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB error analysis\n",
      "negative examples that are mistakenly predicted as positive\n",
      "errors: 0\n",
      "\n",
      "positive examples that are mistakenly predicted as negative\n",
      "'This place used to be great. I can\\'t believe it\\'s current state. Instead of the cool, dimly-lit lounge that I was used to, I was in a cheap, smelly bar. The music has no soul, the bartender is mean. This place no longer exudes a welcoming spirit. The crowd is awkward and old. I want my old hangout back!!'\n",
      "'This diner was not at all up to par. I\\'ve been to many diners, and get eggs benedict sometimes. There was nacho cheese on my eggs, and a plateful of watery runny eggs. And it smelled like smoke. And there was no heat, in the dead of winter. Their prices are not ANYWHERE near what is reasonable. Cool mom & pop place, but terrible food, smell, and prices.'\n",
      "errors: 2\n"
     ]
    }
   ],
   "source": [
    "# print out specific type of error for further analysis\n",
    "\n",
    "# print out the negative examples that are mistakenly predicted as positive\n",
    "# according to the confusion matrix, there should be 0 such examples\n",
    "print(\"MNB error analysis\")\n",
    "print(\"negative examples that are mistakenly predicted as positive\")\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i] == 'p' and y_pred_mnb[i] == 'n'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt + 1\n",
    "print(\"errors:\", err_cnt)\n",
    "\n",
    "# print out the positive examples that are mistakenly predicted as negative\n",
    "# according to the confusion matrix, there should be 2 such examples\n",
    "print()\n",
    "print(\"positive examples that are mistakenly predicted as negative\")\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i] == 'n' and y_pred_mnb[i] == 'p'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt + 1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02a8af",
   "metadata": {},
   "source": [
    "Since the results match with what we expected from the confusion matrix, the errors look correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28304fe6",
   "metadata": {},
   "source": [
    "# Step 5.2 Error Analysis - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "68b217f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM error analysis\n",
      "negative examples that are mistakenly predicted as positive\n",
      "'the staff at this restaurant is very unfriendly. the waitress for our table is extremely rude. we need to wait for one hour for our order to come. the place is noisy and the food isn\\'t that good.'\n",
      "errors: 1\n",
      "positive examples that are mistakenly predicted as negative\n",
      "'This place was one of the best restaurant I have been. The price is little expensive, but the food and the service is best around the area. I went here with my family, and we ordered 4 dishes. They were all well cooked, and their taste were nicely balanced. Waiters came when we needed them without having to call for them. I would definitely recommend it to everyone visiting this area. '\n",
      "'I ate at this restaurant called Banana Leaf. As I entered the restaurant I really liked the ambiance. I ordered noodle soup and fried rice with spicy black bean curry. The service was pretty fast and the food tasted amazing. There was a lot flavor in the food which I truly enjoyed. Two thumbs up for Banana Leaf and I would totally recommend this restaurant.'\n",
      "'I went to this ultra-luxurious restaurant in Downtown New York which is known for its exotic and expensive cuisine. I had a glass of champagne along with very expensive Caviar. I had a delicious Chicken Pasta cooked in white sauce. This was followed by mouth melting chocolate brownie and vanilla ice cream. The service standards were superlative and I felt special visiting this restaurant. '\n",
      "errors: 3\n"
     ]
    }
   ],
   "source": [
    "# print out specific type of error for further analysis\n",
    "\n",
    "# print out the negative examples that are mistakenly predicted as positive\n",
    "# according to the confusion matrix, there should be 1 such examples\n",
    "print(\"SVM error analysis\")\n",
    "print(\"negative examples that are mistakenly predicted as positive\")\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i] == 'n' and y_pred_svm[i] == 'p'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)\n",
    "print()\n",
    "# print out the positive examples that are mistakenly predicted as negative\n",
    "# according to the confusion matrix, there should be 3 such examples\n",
    "# print(\"MNB error analysis\")\n",
    "print(\"positive examples that are mistakenly predicted as negative\")\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i] == 'p' and y_pred_svm[i] == 'n'):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1feffec",
   "metadata": {},
   "source": [
    "# Step 6: write the prediction output to file - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a3bba60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mnb = nb_clf.predict(X_test_vec)\n",
    "output = open('/Users/shivangi/Documents/prediction_output_mnb.csv', 'w')\n",
    "for x, value in enumerate(y_pred_mnb):\n",
    "  output.write(str(value) + '\\n') \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829422b9",
   "metadata": {},
   "source": [
    "# Step 6: write the prediction output to file - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "66f63ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm_clf.predict(X_test_vec)\n",
    "output = open('/Users/shivangi/Documents/prediction_output_svm.csv', 'w')\n",
    "for x, value in enumerate(y_pred_svm):\n",
    "  output.write(str(value) + '\\n') \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a505b",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6318a92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8801169590643274\n",
      "0.7935483870967742\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding = 'latin-1', binary = False)),('nb', MultinomialNB())])\n",
    "scores_mnb = cross_val_score(nb_clf_pipe, X, y, cv = 5)\n",
    "avg_mnb = sum(scores_mnb)/len(scores_mnb)\n",
    "print(avg_mnb)\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary = False)),('svm', LinearSVC(C=1))])\n",
    "scores_svm = cross_val_score(svm_clf_pipe, X, y, cv=3)\n",
    "avg_svm = sum(scores_svm)/len(scores_svm)\n",
    "print(avg_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3ee30",
   "metadata": {},
   "source": [
    "# Repeat all steps for predicting the authenticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a241ad8",
   "metadata": {},
   "source": [
    "# Step 2 - Split train/test data for hold-out test and Step 2.1 - Data Checking - MNB and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "44db6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73,) (73,) (19,) (19,)\n",
      "'I went to Applebees (regrettably) once and it was a train-wreck. The server was in a terrible mood, the beers arrived after the dinner was delivered, the appetizer was wrong, food was bad, the check was wrong, and there were barely any other people inside! I was pretty baffled at how so many things just went wrong in the whole process, the restaurant wasn\\'t even busy. '\n",
      "t\n",
      "'After I went shopping with some of my friend, we went to DODO restaurant for dinner. I found worm in one of the dishes .'\n",
      "f\n",
      "[['f' 't']\n",
      " [36 37]]\n",
      "[['f' 't']\n",
      " [10 9]]\n"
     ]
    }
   ],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X1_train.shape, y1_train.shape, X1_test.shape, y1_test.shape)\n",
    "print(X1_train[0])\n",
    "print(y1_train[0])\n",
    "print(X1_test[0])\n",
    "print(y1_test[0])\n",
    "\n",
    "unique1, counts1 = np.unique(y1_train, return_counts = True)\n",
    "print(np.asarray((unique1, counts1)))\n",
    "\n",
    "uniqueTest1, countsTest1 = np.unique(y1_test, return_counts=True)\n",
    "print(np.asarray((uniqueTest1, countsTest1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581bff9",
   "metadata": {},
   "source": [
    "trainRatio = 36/37 = 97.3% testRatio = 10/9 = 111.11%\n",
    "\n",
    "It's a bit lopsided in test data but that should not be a problem because training data has almost 1:1 negative and positive records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3339a6",
   "metadata": {},
   "source": [
    "We'll use boolean vectorizer for authenticity because Professor mentioned that she has seen it produce better results in short datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe858471",
   "metadata": {},
   "source": [
    "# Step 3.1: Vectorize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ab696838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 70)\n",
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0]]\n",
      "70\n",
      "[('went', 68), ('terrible', 59), ('dinner', 16), ('food', 23), ('bad', 3), ('people', 45), ('just', 31), ('restaurant', 50), ('wasn', 67), ('friends', 26)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "X1_train_vec = unigram_bool_vectorizer.fit_transform(X1_train)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X1_train_vec.shape)\n",
    "print(X1_train_vec[0].toarray())\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_bool_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_bool_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check word index in vocabulary\n",
    "print(unigram_bool_vectorizer.vocabulary_.get('imaginative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fbfc47a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 70)\n"
     ]
    }
   ],
   "source": [
    "X1_test_vec = unigram_bool_vectorizer.transform(X1_test)\n",
    "\n",
    "# print out #examples and #features in the test set\n",
    "print(X1_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066dea00",
   "metadata": {},
   "source": [
    "# Step 4: Train a MNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "04d95e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the MNB model\n",
    "nb_clf1 = MultinomialNB()\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "nb_clf1.fit(X1_train_vec, y1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45087581",
   "metadata": {},
   "source": [
    "# Step 4: Train a SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a46e3005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the LinearSVC model\n",
    "svm_clf1 = LinearSVC(C=1)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf1.fit(X1_train_vec,y1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa53aea",
   "metadata": {},
   "source": [
    "# Step 4.1 Interpret a trained MNB model - conditional probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6d83502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 70)\n",
      "3\n",
      "-4.711779921046349\n",
      "-4.135166556742355\n"
     ]
    }
   ],
   "source": [
    "print(nb_clf1.feature_log_prob_.shape)\n",
    "\n",
    "print(unigram_bool_vectorizer.vocabulary_.get('bad'))\n",
    "\n",
    "# for i in range(0,1):\n",
    "print(nb_clf1.feature_log_prob_[0][unigram_bool_vectorizer.vocabulary_.get('bad')])\n",
    "print(nb_clf1.feature_log_prob_[1][unigram_bool_vectorizer.vocabulary_.get('bad')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265980a",
   "metadata": {},
   "source": [
    "# Step 4.1 Interpret the trained linear SVC model - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b8220a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false words\n",
      "(-1.4959444455936277, 'want')\n",
      "(-1.3149676756434405, 'delicious')\n",
      "(-1.2538386751664088, 'cold')\n",
      "(-1.0948356958177885, 'high')\n",
      "(-0.8856203894599162, 'meal')\n",
      "(-0.8744710437247227, 'worth')\n",
      "(-0.8472953732806963, 'said')\n",
      "(-0.8312008757223224, 'definitely')\n",
      "(-0.6775050231530999, 'dining')\n",
      "(-0.6311361753681147, 'sauce')\n",
      "\n",
      "true words\n",
      "(0.4438421404653148, 'ordered')\n",
      "(0.44657414108890164, 'dish')\n",
      "(0.45984392656574846, 'overall')\n",
      "(0.5298982521167177, 'bad')\n",
      "(0.5358867020691211, 'amazing')\n",
      "(0.6684545296157696, 'came')\n",
      "(0.7259193693973914, 'ask')\n",
      "(0.8004103257861921, 'bar')\n",
      "(0.8334812232211545, 'good')\n",
      "(0.9607665175274639, 'chicken')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## For category \"0\" (negative), get all features and their weights and sort them in increasing order\n",
    "auth_feature_ranks = sorted(zip(svm_clf1.coef_[0], unigram_bool_vectorizer.get_feature_names_out()))\n",
    "\n",
    "## get the 10 features that are best indicators of negative sentiment (they are at the bottom of the ranked list negative_feature_ranks)\n",
    "false_10 = auth_feature_ranks[:10]\n",
    "print(\"false words\")\n",
    "for i in range(0, len(false_10)):\n",
    "    print(false_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are best indicators of positive sentiment (they are at the bottom of the ranked list positive_feature_ranks)\n",
    "true_10 = auth_feature_ranks[-10:]\n",
    "print(\"true words\")\n",
    "for i in range(0, len(true_10)):\n",
    "    print(true_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a96e0b",
   "metadata": {},
   "source": [
    "# Step 4.2 Log ratio of conditional probs - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2ae9a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1.0874389880699846, 'bar'), (-1.0874389880699846, 'environment'), (-1.018446116583033, 'people'), (-0.7465124010993911, 'good'), (-0.68197387996182, 'life'), (-0.6411518854415648, 'time'), (-0.5766133643039941, 'bad'), (-0.5766133643039941, 'nice'), (-0.5766133643039941, 'waitress'), (-0.4588303286476103, 'ask')]\n",
      "[(0.7451424756783256, 'high'), (0.7451424756783256, 'meal'), (0.7451424756783256, 'sauce'), (0.7451424756783256, 'waiters'), (0.7451424756783256, 'worth'), (0.9274640324722805, 'delicious'), (0.9274640324722805, 'staff'), (1.0816147122995385, 'said'), (1.6206112130322259, 'cold'), (1.6206112130322259, 'want')]\n"
     ]
    }
   ],
   "source": [
    "log_ratios1 = []\n",
    "features1 = unigram_bool_vectorizer.get_feature_names_out()\n",
    "false_cond_prob = nb_clf1.feature_log_prob_[0]\n",
    "truth_cond_prob = nb_clf1.feature_log_prob_[1]\n",
    "\n",
    "for i in range(0, len(features1)):\n",
    "  log_ratio1 = false_cond_prob[i] - truth_cond_prob[i]\n",
    "  log_ratios1.append(log_ratio1)\n",
    "\n",
    "exercise_C_ranks1 = sorted(zip(log_ratios1, features1))\n",
    "print(exercise_C_ranks1[:10])\n",
    "print(exercise_C_ranks1[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cd9d7",
   "metadata": {},
   "source": [
    "# Step 5: Test the MNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "18cff9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47368421052631576"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "\n",
    "nb_clf1.score(X1_test_vec, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "11b045bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 3]\n",
      " [7 2]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix (row: ground truth; col: prediction)\n",
    "\n",
    "y1_pred_mnb = nb_clf1.fit(X1_train_vec, y1_train).predict(X1_test_vec)\n",
    "cm1_mnb = confusion_matrix(y1_test, y1_pred_mnb, labels = ['f', 't'])\n",
    "print(cm1_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "73d85dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.4]\n",
      "[0.7        0.22222222]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.70      0.58        10\n",
      "           1       0.40      0.22      0.29         9\n",
      "\n",
      "    accuracy                           0.47        19\n",
      "   macro avg       0.45      0.46      0.43        19\n",
      "weighted avg       0.45      0.47      0.44        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "\n",
    "print(precision_score(y1_test, y1_pred_mnb, average = None))\n",
    "print(recall_score(y1_test, y1_pred_mnb, average = None))\n",
    "\n",
    "target_names1 = ['0','1']\n",
    "print(classification_report(y1_test, y1_pred_mnb, target_names = target_names1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fed56c",
   "metadata": {},
   "source": [
    "# Step 5: Test the LinearSVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "84ce8872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3684210526315789"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "\n",
    "svm_clf1.score(X1_test_vec,y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5a50acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5]\n",
      " [7 2]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.50      0.45        10\n",
      "           1       0.29      0.22      0.25         9\n",
      "\n",
      "    accuracy                           0.37        19\n",
      "   macro avg       0.35      0.36      0.35        19\n",
      "weighted avg       0.35      0.37      0.36        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "\n",
    "y1_pred_svm = svm_clf1.predict(X1_test_vec)\n",
    "cm1_svm = confusion_matrix(y1_test, y1_pred_svm, labels = ['f', 't'])\n",
    "print(cm1_svm)\n",
    "print()\n",
    "\n",
    "target_names = ['0','1']\n",
    "print(classification_report(y1_test, y1_pred_svm, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07145c08",
   "metadata": {},
   "source": [
    "# Step 5.1 Interpret the prediction result - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "585b260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43542296 0.56457704]\n",
      "t\n",
      "f\n"
     ]
    }
   ],
   "source": [
    "## find the calculated posterior probability\n",
    "posterior_probs1_mnb = nb_clf1.predict_proba(X1_test_vec)\n",
    "\n",
    "## find the posterior probabilities for the first test example\n",
    "print(posterior_probs1_mnb[0])\n",
    "\n",
    "# find the category prediction for the first test example\n",
    "y1_pred_mnb = nb_clf1.predict(X1_test_vec)\n",
    "print(y1_pred_mnb[0])\n",
    "\n",
    "# check the actual label for the first test example\n",
    "print(y1_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9887016",
   "metadata": {},
   "source": [
    "Because the posterior probability for category 'f' (false) is the greatest, 0.565, the prediction should be \"f\". Because the actual label is \"t\", this is not as accurate of a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da93051",
   "metadata": {},
   "source": [
    "# Step 5.1 Interpret the prediction result - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "58a4010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10491385536768938\n",
      "f\n",
      "'Mike\\'s Pizza High Point, NY Service was very slow and the quality was low. You would think they would know at least how to make good pizza, not. Stick to pre-made dishes like stuffed pasta or a salad. You should consider dining else where.'\n"
     ]
    }
   ],
   "source": [
    "## get the confidence scores for all test examples from each of the five binary classifiers\n",
    "svm_confidence_scores1 = svm_clf1.decision_function(X1_test_vec)\n",
    "## get the confidence score for the first test example\n",
    "print(svm_confidence_scores1[0])\n",
    "\n",
    "## Confirm by printing out the actual prediction\n",
    "print(y1_test[0])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "08e762a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53454464, 0.46545536])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_calibrated1 = CalibratedClassifierCV(svm_clf1) \n",
    "svm_calibrated1.fit(X1_train_vec, y1_train)\n",
    "y1_test_proba_svm = svm_calibrated1.predict_proba(X1_test_vec)\n",
    "y1_test_proba_svm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa3317",
   "metadata": {},
   "source": [
    "# Step 5.2 Error Analysis - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e76e09c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'After I went shopping with some of my friend, we went to DODO restaurant for dinner. I found worm in one of the dishes .'\n",
      "'I ate at this restaurant called Banana Leaf. As I entered the restaurant I really liked the ambiance. I ordered noodle soup and fried rice with spicy black bean curry. The service was pretty fast and the food tasted amazing. There was a lot flavor in the food which I truly enjoyed. Two thumbs up for Banana Leaf and I would totally recommend this restaurant.'\n",
      "'OMG. This restaurant is horrible. The receptionist did not greet us, we just stood there and waited for five minutes. The food came late and served not warm. Me and my pet ordered a bowl of salad and a cheese pizza. The salad was not fresh, the crust of a pizza was so hard like plastics. My dog didn\\'t even eat that pizza. I hate this place!!!!!!!!!!'\n",
      "errors: 3\n",
      "\n",
      "'This place used to be great. I can\\'t believe it\\'s current state. Instead of the cool, dimly-lit lounge that I was used to, I was in a cheap, smelly bar. The music has no soul, the bartender is mean. This place no longer exudes a welcoming spirit. The crowd is awkward and old. I want my old hangout back!!'\n",
      "'the staff at this restaurant is very unfriendly. the waitress for our table is extremely rude. we need to wait for one hour for our order to come. the place is noisy and the food isn\\'t that good.'\n",
      "'I went to this awesome restaurant in San Francisco (I forget the name), but it was on point. Huge beer list, quick seating, the menu was long but not over-whelming with great variety and unique options, and the staff was very friendly. They played great music the whole time, and the food was delicious. We ended up hanging out at the bar (the west coast has the best IPAs!) for a few hours after what was originally going to be a quick lunch, then went to a Phish show, pretty awesome day. '\n",
      "'The worst restaurant that I have ever eaten in is undoubtedly this place called \\'Kneed and feed\\'. I ordered a veggie sandwich and a red pepper soup. When the waitress placed my order on the table it looked very promising-- but when I took a bite of my sandwich which was also a task as the bread was too hard to bite into, the veggie burger had no flavor. Also the red pepper soup was very bland and I had to add a lot of salt and pepper to give it some flavor. Even though I was starving I was unable to finish my order. '\n",
      "'This diner was not at all up to par. I\\'ve been to many diners, and get eggs benedict sometimes. There was nacho cheese on my eggs, and a plateful of watery runny eggs. And it smelled like smoke. And there was no heat, in the dead of winter. Their prices are not ANYWHERE near what is reasonable. Cool mom & pop place, but terrible food, smell, and prices.'\n",
      "'Halo\\'s is home. I have been here numerous times. The staff knows my name. That\\'s the feeling I keep coming back to. The coffee is great (any time of the day, you\\'ll always get fresh coffee). They have little munchies in case you\\'re not looking to have a full meal. A great place to have a quiet afternoon by yourself and catch up on some reading, or catch up with an old friend over never-ending conversations.'\n",
      "'Pastablities is a locally owned restaurant in Syracuse. The food is simple and homey and comforting. Their famous bread is baked daily and the bakery is right next door. The bread is soft and chewy and amazing with their homemade spicy tomato sauce. The paste and cheese that I had was cream and cooked to perfection. '\n",
      "errors: 7\n"
     ]
    }
   ],
   "source": [
    "# print out specific type of error for further analysis\n",
    "\n",
    "# print out the false examples that are mistakenly predicted as true\n",
    "# according to the confusion matrix, there should be 3 such examples\n",
    "\n",
    "err_cnt1 = 0\n",
    "for i in range(0, len(y1_test)):\n",
    "    if(y1_test[i] == 'f' and y1_pred_mnb[i] == 't'):\n",
    "        print(X1_test[i])\n",
    "        err_cnt1 = err_cnt1 + 1\n",
    "print(\"errors:\", err_cnt1)\n",
    "print()\n",
    "# print out the true examples that are mistakenly predicted as false\n",
    "# according to the confusion matrix, there should be 7 such examples\n",
    "\n",
    "err_cnt1 = 0\n",
    "for i in range(0, len(y1_test)):\n",
    "    if(y1_test[i] == 't' and y1_pred_mnb[i] == 'f'):\n",
    "        print(X1_test[i])\n",
    "        err_cnt1 = err_cnt1 + 1\n",
    "print(\"errors:\", err_cnt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7014063",
   "metadata": {},
   "source": [
    "Since the results match with what we expected from the confusion matrix, the errors look correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06499d4c",
   "metadata": {},
   "source": [
    "# Step 5.2 - Error Analysis - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5dcc5104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM error analysis\n",
      "false examples that are mistakenly predicted as true\n",
      "'I recently ate at a restaurant called White Castle and it was a dine in. I had to wait 20 minutes before the waiter came to my table to take my order even though it was not busy. I had to wait another 30 minutes for my order to come. I had ordered a veggie burger with fries and Iced tea. The veggie patty was not properly cooked, the lettuce had black patches and the tomatoes looked horrible. Overall the burger was a disaster. When I turned to the fries in the hopes that it would taste good--to my disappointment they were also not fried properly and had a raw taste. To top it all when picked up my iced tea took take a sip-- I found a fly swimming in my iced tea. Overall it was a really bad experience and I would not recommend White Castle to anybody. '\n",
      "'I went there with two friends at 6pm. Long queue was there. But it didn\\'t take us long to wait. The waiter was nice but worked in a hurry. We ordered \\'Today\\'s Special\\', some drinks and two icecreams. I had a steak, a little bit too salty, but acceptable. My friends didn\\'t like their lamb chop and cod filet that much. It costed us almost $100. Not worth it. Will not visit there any more.'\n",
      "'Carlo\\'s Plate Shack was the worst dining experience of my life. Although my Southern Comfort Plate sounded to die for, the staff was extremely unhelpful at every turn. We started off with drinks, I had a sick Loganberry milkshake, and my friends had fresh brewed, but bland, iced tea (the ice likely melted and diluted). Eventually our server returned a half hour later to take our orders. I had the aforementioned Southern Comfort Plate, while my friends ordered the Buffalo Chicken Plate and the Hawaiian Plate Lunch. The Southern Comfort Plate came out first, a good 15 minutes before the others, and was extremely greasy. The other 2 ended up being nearly room temperature when they came out. Our server failed to return again to check on us until she brought our check rather abruptly. We want to give this place a chance, but it\\'s rather difficult to subject ourselves to such brutal service and pay money.'\n",
      "'I ate at this restaurant called Banana Leaf. As I entered the restaurant I really liked the ambiance. I ordered noodle soup and fried rice with spicy black bean curry. The service was pretty fast and the food tasted amazing. There was a lot flavor in the food which I truly enjoyed. Two thumbs up for Banana Leaf and I would totally recommend this restaurant.'\n",
      "'OMG. This restaurant is horrible. The receptionist did not greet us, we just stood there and waited for five minutes. The food came late and served not warm. Me and my pet ordered a bowl of salad and a cheese pizza. The salad was not fresh, the crust of a pizza was so hard like plastics. My dog didn\\'t even eat that pizza. I hate this place!!!!!!!!!!'\n",
      "errors: 5\n",
      "\n",
      "positive examples that are mistakenly predicted as negative\n",
      "'This place used to be great. I can\\'t believe it\\'s current state. Instead of the cool, dimly-lit lounge that I was used to, I was in a cheap, smelly bar. The music has no soul, the bartender is mean. This place no longer exudes a welcoming spirit. The crowd is awkward and old. I want my old hangout back!!'\n",
      "'the staff at this restaurant is very unfriendly. the waitress for our table is extremely rude. we need to wait for one hour for our order to come. the place is noisy and the food isn\\'t that good.'\n",
      "'I went to this awesome restaurant in San Francisco (I forget the name), but it was on point. Huge beer list, quick seating, the menu was long but not over-whelming with great variety and unique options, and the staff was very friendly. They played great music the whole time, and the food was delicious. We ended up hanging out at the bar (the west coast has the best IPAs!) for a few hours after what was originally going to be a quick lunch, then went to a Phish show, pretty awesome day. '\n",
      "'The best restaurant I have gone to is when I went to AppleBee with my friends, the service there is so nice. Food is delicious, I liked the steak very much! The environment is very nice and clean, which makes me want to go there for more times. Also, the feeling when talking with my friends at such a good restaurant after skiing is wonderful. I want to go there again!!'\n",
      "'The worst restaurant that I have ever eaten in is undoubtedly this place called \\'Kneed and feed\\'. I ordered a veggie sandwich and a red pepper soup. When the waitress placed my order on the table it looked very promising-- but when I took a bite of my sandwich which was also a task as the bread was too hard to bite into, the veggie burger had no flavor. Also the red pepper soup was very bland and I had to add a lot of salt and pepper to give it some flavor. Even though I was starving I was unable to finish my order. '\n",
      "'Halo\\'s is home. I have been here numerous times. The staff knows my name. That\\'s the feeling I keep coming back to. The coffee is great (any time of the day, you\\'ll always get fresh coffee). They have little munchies in case you\\'re not looking to have a full meal. A great place to have a quiet afternoon by yourself and catch up on some reading, or catch up with an old friend over never-ending conversations.'\n",
      "'Restaurant : Samrat Food Ordered : Dal Tadka, Baigan Bharta, Kadhai Paneer, Jaljeera The food ordered was scrumptiously delicious at the place we went on a Saturday night, we went around 6 pm at this place we a bunch of 6 people. The food came in just 10 minutes. The environment at this place was wonderful, it had soothing hindi tunes playing in the background. The food we ordered for sufficient in quantity and taste. Overall Rating 4/5 Try everyday special dish, the prices here are cheap on pocket, this place is generally packed so reserve a table over a call before you dine here.'\n",
      "errors: 7\n"
     ]
    }
   ],
   "source": [
    "# print out specific type of error for further analysis\n",
    "\n",
    "# print out the false examples that are mistakenly predicted as true\n",
    "# according to the confusion matrix, there should be 5 such examples\n",
    "print(\"SVM error analysis\")\n",
    "print(\"false examples that are mistakenly predicted as true\")\n",
    "err_cnt1 = 0\n",
    "for i in range(0, len(y1_test)):\n",
    "    if(y1_test[i] == 'f' and y1_pred_svm[i] == 't'):\n",
    "        print(X1_test[i])\n",
    "        err_cnt1 = err_cnt1 + 1\n",
    "print(\"errors:\", err_cnt1)\n",
    "print()\n",
    "# print out the true examples that are mistakenly predicted as false\n",
    "# according to the confusion matrix, there should be 7 such examples\n",
    "# print(\"MNB error analysis\")\n",
    "print(\"true examples that are mistakenly predicted as false\")\n",
    "err_cnt1 = 0\n",
    "for i in range(0, len(y1_test)):\n",
    "    if(y1_test[i] == 't' and y1_pred_svm[i] == 'f'):\n",
    "        print(X1_test[i])\n",
    "        err_cnt1 = err_cnt1 + 1\n",
    "print(\"errors:\", err_cnt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66191558",
   "metadata": {},
   "source": [
    "# Step 6: write the prediction output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "86134b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred_mnb = nb_clf1.predict(X1_test_vec)\n",
    "output1 = open('/Users/shivangi/Documents/prediction_output_auth_mnb.csv', 'w')\n",
    "for x, value in enumerate(y1_pred_mnb):\n",
    "  output1.write(str(value) + '\\n') \n",
    "output1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03b990",
   "metadata": {},
   "source": [
    "# Step 6: write the prediction output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ae126170",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred_svm = svm_clf1.predict(X1_test_vec)\n",
    "output1 = open('/Users/shivangi/Documents/prediction_output_auth_svm.csv', 'w')\n",
    "for x, value in enumerate(y1_pred_svm):\n",
    "  output1.write(str(value) + '\\n') \n",
    "output1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063ea6f",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7b8798ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8251461988304094\n",
      "0.8043010752688172\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "nb_clf_pipe1 = Pipeline([('vect', CountVectorizer(encoding = 'latin-1', binary = True)),('nb', MultinomialNB())])\n",
    "scores1 = cross_val_score(nb_clf_pipe1, X, y, cv = 5)\n",
    "avg1 = sum(scores1)/len(scores1)\n",
    "print(avg1)\n",
    "\n",
    "svm_clf_pipe1 = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary = True)),('svm', LinearSVC(C=1))])\n",
    "scores_svm1 = cross_val_score(svm_clf_pipe1, X, y, cv = 3)\n",
    "avg_svm1 = sum(scores_svm1)/len(scores_svm1)\n",
    "print(avg_svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd93f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
