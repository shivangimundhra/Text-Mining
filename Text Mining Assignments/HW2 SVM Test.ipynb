{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - build LinearSVC model with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use the Sci-kit Learn (sklearn) package to build linearSVC model, rank features, and use the model for prediction. We will be using the Kaggle sentiment data again.\n",
    "\n",
    "Note that sklearn actually provides two SVM algorithms: SVC and LinearSVC. \n",
    "\n",
    "The SVC module allows for choosing nonlinear kernels, and it uses one-vs-one strategy for multi-class classification.\n",
    "\n",
    "The LinearSVC module uses the linear kernel, and it uses one-vs-all strategy for multi-class classification, so linearSVC is generally faster than SVC. Since linear kernel works better for text classification in general, this tutorial demonstrates how to use LinearSVC for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# read in the training data\n",
    "# the data set includes four columns: PhraseId, SentenceId, Phrase, Sentiment\n",
    "# In this data set a sentence is further split into phrases \n",
    "# in order to build a sentiment classification model\n",
    "# that can not only predict sentiment of sentences but also shorter phrases\n",
    "\n",
    "# A data example:\n",
    "# PhraseId SentenceId Phrase Sentiment\n",
    "# 1 1 A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .1\n",
    "\n",
    "# the Phrase column includes the training examples\n",
    "# the Sentiment column includes the training labels\n",
    "# \"0\" for very negative\n",
    "# \"1\" for negative\n",
    "# \"2\" for neutral\n",
    "# \"3\" for positive\n",
    "# \"4\" for very positive\n",
    "\n",
    "\n",
    "import pandas as p\n",
    "import numpy as np\n",
    "train = p.read_csv(\"/Users/shivangi/Downloads/deception_data_converted_final(1).tsv\", delimiter='\\t')\n",
    "# y = train['sentiment'].values\n",
    "# y1 = train['lie'].values\n",
    "# X = train['review'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['sentiment'] == 'n', 'Sentiments'] = 0\n",
    "train.loc[train['sentiment'] == 'p', 'Sentiments'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lie</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'Mike\\'s Pizza High Point, NY Service was very...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'i really like this buffet restaurant in Marsh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'After I went shopping with some of my friend,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'Olive Oil Garden was very disappointing. I ex...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'The Seven Heaven restaurant was never known f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lie sentiment                                             review  Sentiments\n",
       "0   f         n  'Mike\\'s Pizza High Point, NY Service was very...         0.0\n",
       "1   f         n  'i really like this buffet restaurant in Marsh...         0.0\n",
       "2   f         n  'After I went shopping with some of my friend,...         0.0\n",
       "3   f         n  'Olive Oil Garden was very disappointing. I ex...         0.0\n",
       "4   f         n  'The Seven Heaven restaurant was never known f...         0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Sentiments'].values\n",
    "y1 = train['lie'].values\n",
    "X = train['review'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Split train/test data for hold-out test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73,) (73,) (19,) (19,)\n",
      "'I went to Applebees (regrettably) once and it was a train-wreck. The server was in a terrible mood, the beers arrived after the dinner was delivered, the appetizer was wrong, food was bad, the check was wrong, and there were barely any other people inside! I was pretty baffled at how so many things just went wrong in the whole process, the restaurant wasn\\'t even busy. '\n",
      "0.0\n",
      "'After I went shopping with some of my friend, we went to DODO restaurant for dinner. I found worm in one of the dishes .'\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# check the sklearn documentation for train_test_split\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# \"test_size\" : float, int, None, optional\n",
    "# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "# If int, represents the absolute number of test samples. \n",
    "# If None, the value is set to the complement of the train size. \n",
    "# By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if train_size is unspecified, otherwise it will complement the specified train_size.    \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output from the code above:\n",
    "\n",
    "(93636,) (93636,) (62424,) (62424,)\n",
    "almost in a class with that of Wilde\n",
    "3\n",
    "escape movie\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.1 Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [35. 38.]]\n"
     ]
    }
   ],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# Check how many training examples in each category\n",
    "# this is important to see whether the data set is balanced or skewed\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample output shows that the data set is skewed with 47718/93636=51% \"neutral\" examples. All other categories are smaller.\n",
    "\n",
    "{0, 1, 2, 3, 4}\n",
    "[[    0  4141]\n",
    " [    1 16449]\n",
    " [    2 47718]\n",
    " [    3 19859]\n",
    " [    4  5469]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# sklearn contains two vectorizers\n",
    "\n",
    "# CountVectorizer can give you Boolean or TF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# TfidfVectorizer can give you TF or TFIDF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "# Read the sklearn documentation to understand all vectorization options\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# several commonly used vectorizer setting\n",
    "\n",
    "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "\n",
    "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=5, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Vectorize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 70)\n",
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 2 0]]\n",
      "70\n",
      "[('went', 68), ('terrible', 59), ('dinner', 16), ('food', 23), ('bad', 3), ('people', 45), ('just', 31), ('restaurant', 50), ('wasn', 67), ('friends', 26)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# The vectorizer can do \"fit\" and \"transform\"\n",
    "# fit is a process to collect unique tokens into the vocabulary\n",
    "# transform is a process to convert each document to vector based on the vocabulary\n",
    "# These two processes can be done together using fit_transform(), or used individually: fit() or transform()\n",
    "\n",
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec.shape)\n",
    "print(X_train_vec[0].toarray())\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(unigram_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_count_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check word index in vocabulary\n",
    "print(unigram_count_vectorizer.vocabulary_.get('imaginative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "(93636, 11967)\n",
    "[[0 0 0 ..., 0 0 0]]\n",
    "11967\n",
    "[('imaginative', 5224), ('tom', 10809), ('smiling', 9708), ('easy', 3310), ('diversity', 3060), ('impossibly', 5279), ('buy', 1458), ('sentiments', 9305), ('households', 5095), ('deteriorates', 2843)]\n",
    "5224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Vectorize the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 70)\n"
     ]
    }
   ],
   "source": [
    "# this step is the same as the NB script\n",
    "\n",
    "# use the vocabulary constructed from the training data to vectorize the test data. \n",
    "# Therefore, use \"transform\" only, not \"fit_transform\", \n",
    "# otherwise \"fit\" would generate a new vocabulary from the test data\n",
    "\n",
    "X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "\n",
    "# print out #examples and #features in the test set\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "(62424, 14324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train a LinearSVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the LinearSVC module\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# initialize the LinearSVC model\n",
    "svm_clf = LinearSVC(C=1)\n",
    "\n",
    "# use the training data to train the model\n",
    "svm_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.1 Interpret a trained LinearSVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative words\n",
      "(0.353880898499828, 'ask')\n",
      "(0.3888561784431786, 'environment')\n",
      "(0.45739450890051025, 'waiters')\n",
      "(0.46621912582440517, 'great')\n",
      "(0.5612568776517189, 'fresh')\n",
      "(0.5899584909827561, 'delicious')\n",
      "(0.6048687090652864, 'nice')\n",
      "(0.6437211970258859, 'friendly')\n",
      "(0.7862010356749219, 'need')\n",
      "(0.9646190205193345, 'best')\n",
      "\n",
      "not very negative words\n",
      "(-0.7523672686874645, 'cold')\n",
      "(-0.6548030711996905, 'dishes')\n",
      "(-0.629116183817397, 'service')\n",
      "(-0.5566384777295768, 'said')\n",
      "(-0.533690262001584, 'went')\n",
      "(-0.5216376439264832, 'meal')\n",
      "(-0.5008968355572891, 'asked')\n",
      "(-0.4879109847043264, 'ny')\n",
      "(-0.46825282529132795, 'minutes')\n",
      "(-0.4550554333175944, 'restaurant')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## interpreting LinearSVC models\n",
    "## http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "## LinearSVC uses a one-vs-all strategy to extend the binary SVM classifier to multi-class problems\n",
    "## for the Kaggle sentiment classification problem, there are five categories 0,1,2,3,4 with 0 as very negative and 4 very positive\n",
    "## LinearSVC builds five binary classifier, \"very negative vs. others\", \"negative vs. others\", \"neutral vs. others\", \"positive vs. others\", \"very positive vs. others\", \n",
    "## and then pick the most confident prediction as the final prediction.\n",
    "\n",
    "## Linear SVC also ranks all features based on their contribution to distinguish the two concepts in each binary classifier\n",
    "## For category \"0\" (very negative), get all features and their weights and sort them in increasing order\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[0], unigram_count_vectorizer.get_feature_names_out()))\n",
    "\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_negative_10 = feature_ranks[-10:]\n",
    "print(\"Very negative words\")\n",
    "for i in range(0, len(very_negative_10)):\n",
    "    print(very_negative_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_negative_10 = feature_ranks[:10]\n",
    "print(\"not very negative words\")\n",
    "for i in range(0, len(not_very_negative_10)):\n",
    "    print(not_very_negative_10[i])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "Very negative words\n",
    "(1.5635285973159958, 'stunning')\n",
    "(1.6005795078256047, 'astonish')\n",
    "(1.610812924790558, 'refreshes')\n",
    "(1.6148905161082145, 'flawless')\n",
    "(1.6474647033330083, 'phenomenal')\n",
    "(1.6506425169734038, 'masterful')\n",
    "(1.67761558779458, 'masterfully')\n",
    "(1.8781421016763864, 'glorious')\n",
    "(1.9801881772566481, 'miraculous')\n",
    "(2.0143251933052397, 'perfection')\n",
    "\n",
    "not very negative words\n",
    "(-2.3147454187985117, 'sacrifices')\n",
    "(-1.8650987318574794, 'maintained')\n",
    "(-1.8305667747223913, 'placed')\n",
    "(-1.7974037295239631, 'argue')\n",
    "(-1.6800998534753624, '19')\n",
    "(-1.6684863939524339, 'homage')\n",
    "(-1.6179084517399509, 'failure')\n",
    "(-1.6088792786048403, 'breezy')\n",
    "(-1.6059138072144292, 'bore')\n",
    "(-1.5466693614369267, 'clone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# write code similar to the above sample code \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# and print out the 10 most indicative words for the \"very positive\" category and the \"positive\" category\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Your code starts here\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m feature_ranks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43msvm_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, unigram_count_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m very_positive_10 \u001b[38;5;241m=\u001b[39m feature_ranks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# write code similar to the above sample code \n",
    "# and print out the 10 most indicative words for the \"very positive\" category and the \"positive\" category\n",
    "\n",
    "# Your code starts here\n",
    "feature_ranks = sorted(zip(svm_clf.coef_[1], unigram_count_vectorizer.get_feature_names_out()))\n",
    "\n",
    "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
    "very_positive_10 = feature_ranks[-10:]\n",
    "print(\"Very positive words\")\n",
    "for i in range(0, len(very_positive_10)):\n",
    "    print(very_positive_10[i])\n",
    "print()\n",
    "\n",
    "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
    "not_very_positive_10 = feature_ranks[:10]\n",
    "print(\"not very positive words\")\n",
    "for i in range(0, len(not_very_positive_10)):\n",
    "    print(not_very_positive_10[i])\n",
    "print()\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 11967)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<93636x11967 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 334671 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 3, ..., 2, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output\n",
    "\n",
    "(1.5635285973159958, 'stunning')\n",
    "(1.6005795078256047, 'astonish')\n",
    "(1.610812924790558, 'refreshes')\n",
    "(1.6148905161082145, 'flawless')\n",
    "(1.6474647033330083, 'phenomenal')\n",
    "(1.6506425169734038, 'masterful')\n",
    "(1.67761558779458, 'masterfully')\n",
    "(1.8781421016763864, 'glorious')\n",
    "(1.9801881772566481, 'miraculous')\n",
    "(2.0143251933052397, 'perfection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Test the LinearSVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6236864026656415"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "\n",
    "svm_clf.score(X_test_vec,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  918  1221   697    82    13]\n",
      " [  701  4080  5504   514    25]\n",
      " [  195  2106 27081  2310   172]\n",
      " [   34   396  6048  5533  1057]\n",
      " [    3    51   590  1772  1321]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.31      0.38      2931\n",
      "           1       0.52      0.38      0.44     10824\n",
      "           2       0.68      0.85      0.75     31864\n",
      "           3       0.54      0.42      0.48     13068\n",
      "           4       0.51      0.35      0.42      3737\n",
      "\n",
      "    accuracy                           0.62     62424\n",
      "   macro avg       0.55      0.46      0.49     62424\n",
      "weighted avg       0.60      0.62      0.60     62424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm=confusion_matrix(y_test, y_pred, labels=[0,1,2,3,4])\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['0','1','2','3','4']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.1 Interpret the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01718403 -0.50760026  0.2233101  -0.97514739 -1.24718853]\n",
      "2\n",
      "A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n"
     ]
    }
   ],
   "source": [
    "## get the confidence scores for all test examples from each of the five binary classifiers\n",
    "svm_confidence_scores = svm_clf.decision_function(X_test_vec)\n",
    "## get the confidence score for the first test example\n",
    "print(svm_confidence_scores[0])\n",
    "\n",
    "## sample output: array([-1.05306321, -0.62746206,  0.31074854, -0.89709483, -1.08343089]\n",
    "## because the confidence score is the highest for category 2, \n",
    "## the prediction should be 2. \n",
    "\n",
    "## Confirm by printing out the actual prediction\n",
    "print(y_test[0])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.03909121, 0.20406249, 0.60822947, 0.11914359, 0.02947324])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output prediction probs\n",
    "# https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "svm_calibrated = CalibratedClassifierCV(svm_clf) \n",
    "svm_calibrated.fit(X_train_vec, y_train)\n",
    "y_test_proba = svm_calibrated.predict_proba(X_test_vec)\n",
    "y_test_proba[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above sample code examines LinearSVC's prediction confidence for the first example.\n",
    "# Now you are going to print out another example with index=99 (the 100th example), \n",
    "# and examine the LinearSVC's prediction confidence and its final prediction\n",
    "# does this prediction seem correct to you?\n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.2 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 minutes into the film you 'll be white-knuckled and unable to look away .\n",
      "the film is never dull\n",
      "greatly impressed by the skill of the actors involved in the enterprise\n",
      "errors: 3\n"
     ]
    }
   ],
   "source": [
    "# print out specific type of error for further analysis\n",
    "\n",
    "# print out the very positive examples that are mistakenly predicted as negative\n",
    "# according to the confusion matrix, there should be 53 such examples\n",
    "# note if you use a different vectorizer option, your result might be different\n",
    "\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i]==4 and y_pred[i]==0):\n",
    "        print(X_test[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to print out \n",
    "# the errors that very negative examples were mistakenly predicted as very positive?\n",
    "# and the errors that very positive examples were mistakenly predicted as very negative?\n",
    "\n",
    "# Try find lingustic patterns for these two types of errors\n",
    "# Based on the above error analysis, what suggestions would you give to improve the current model?\n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "# for more details see https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "# evaluation metrics provided by sklearn - https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "# if you need to output only one metric, such as accuracy or f1_macro, use the \"cross_val_score\" function\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('svm', LinearSVC(C=1))])\n",
    "scores = cross_val_score(svm_clf_pipe, X, y, cv=3, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# if you need to output multiple metrics, such as accuracy and f1_macro, use the \"cross_validate\" function\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['accuracy', 'f1_macro', 'f1_micro', 'precision_macro', 'recall_macro']\n",
    "svm_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('svm', LinearSVC(C=1))])\n",
    "scores = cross_validate(svm_clf_pipe, X, y, cv=3, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fit_time',\n",
       " 'score_time',\n",
       " 'test_accuracy',\n",
       " 'test_f1_macro',\n",
       " 'test_f1_micro',\n",
       " 'test_precision_macro',\n",
       " 'test_recall_macro',\n",
       " 'train_accuracy',\n",
       " 'train_f1_macro',\n",
       " 'train_f1_micro',\n",
       " 'train_precision_macro',\n",
       " 'train_recall_macro']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve scores from a metric\n",
    "def get_metric_scores (scores, metric, train_or_test, verbose=False):\n",
    "    metric_name = train_or_test + '_' + metric\n",
    "    print(metric_name) \n",
    "\n",
    "    metric_scores = scores[metric_name]\n",
    "    if (verbose == True):\n",
    "        print(metric_scores)\n",
    "    avg = sum(metric_scores) / len(metric_scores)\n",
    "    print('average')\n",
    "    avg_formatted = \"{:.3f}\".format(avg)\n",
    "    print(avg_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy\n",
      "[0.57600923 0.56401384 0.56680123]\n",
      "average\n",
      "0.569\n"
     ]
    }
   ],
   "source": [
    "#retrieve test accuracy scores\n",
    "get_metric_scores(scores, 'accuracy', 'test', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy\n",
      "average\n",
      "0.738\n"
     ]
    }
   ],
   "source": [
    "#retrieve training accuracy scores\n",
    "get_metric_scores(scores, 'accuracy', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/byu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# compare performance with different choice of C values\n",
    "\n",
    "svm_clf_pipe2 = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('svm', LinearSVC(C=0.5))])\n",
    "scores2 = cross_validate(svm_clf_pipe2, X, y, cv=3, scoring=scoring, return_train_score=True)\n",
    "\n",
    "svm_clf_pipe3 = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('svm', LinearSVC(C=2))])\n",
    "scores3 = cross_validate(svm_clf_pipe3, X, y, cv=3, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1\n",
      "\n",
      "train_accuracy\n",
      "average\n",
      "0.738\n",
      "test_accuracy\n",
      "average\n",
      "0.569\n",
      "\n",
      "C=0.5\n",
      "\n",
      "train_accuracy\n",
      "average\n",
      "0.732\n",
      "test_accuracy\n",
      "average\n",
      "0.574\n",
      "\n",
      "C=2\n",
      "\n",
      "train_accuracy\n",
      "average\n",
      "0.741\n",
      "test_accuracy\n",
      "average\n",
      "0.565\n"
     ]
    }
   ],
   "source": [
    "# compare the effect of different C values\n",
    "# C=1\n",
    "print('C=1\\n')\n",
    "get_metric_scores(scores, 'accuracy', 'train')\n",
    "get_metric_scores(scores, 'accuracy', 'test')\n",
    "\n",
    "# C=0.5\n",
    "print('\\nC=0.5\\n')\n",
    "get_metric_scores(scores2, 'accuracy', 'train')\n",
    "get_metric_scores(scores2, 'accuracy', 'test')\n",
    "\n",
    "# C=2\n",
    "print('\\nC=2\\n')\n",
    "get_metric_scores(scores3, 'accuracy', 'train')\n",
    "get_metric_scores(scores3, 'accuracy', 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the f1_macro scores and compare the effect of C on f1_macro \n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
